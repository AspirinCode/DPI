{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validate GCNs\n",
    "\n",
    "Train and validate GCNs on DUD-E data subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import os\\nimport time\\nimport torch\\nimport torch.nn.functional as F\\n\\nimport numpy as np\\nimport pandas as pd\\n\\nfrom progressbar import progressbar\\nfrom torch.nn import Linear, Softmax, Dropout\\nfrom LigandDataset import LigandDataset\\nfrom torch_geometric.nn.pool import topk_pool\\nfrom torch_geometric.data import (\\n    Data,\\n)\\nfrom torch_geometric.nn import GCNConv, global_mean_pool\\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score\";\n",
       "                var nbb_formatted_code = \"import os\\nimport time\\nimport torch\\nimport torch.nn.functional as F\\n\\nimport numpy as np\\nimport pandas as pd\\n\\nfrom progressbar import progressbar\\nfrom torch.nn import Linear, Softmax, Dropout\\nfrom LigandDataset import LigandDataset\\nfrom torch_geometric.nn.pool import topk_pool\\nfrom torch_geometric.data import (\\n    Data,\\n)\\nfrom torch_geometric.nn import GCNConv, global_mean_pool\\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from progressbar import progressbar\n",
    "from torch.nn import Linear, Softmax, Dropout\n",
    "from LigandDataset import LigandDataset\n",
    "from torch_geometric.nn.pool import topk_pool\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    ")\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# Gradient descent batch size.\\nBATCH_SIZE = 32\\n# Number of training epochs.\\nNUM_EPOCHS = 10\\n# Number of convolutional blocks.\\nNUM_BLOCKS_LIGANDS = 1\\nNUM_BLOCKS_PROTEINS = 1\\n# Number of convolutional channels.\\nNUM_CHANNELS_LIGANDS = 16\\nNUM_CHANNELS_PROTEINS = 16\\n# Graph power to create bonds with after pooling.\\nGRAPH_POWER_AFTER_POOLING = 2\";\n",
       "                var nbb_formatted_code = \"# Gradient descent batch size.\\nBATCH_SIZE = 32\\n# Number of training epochs.\\nNUM_EPOCHS = 10\\n# Number of convolutional blocks.\\nNUM_BLOCKS_LIGANDS = 1\\nNUM_BLOCKS_PROTEINS = 1\\n# Number of convolutional channels.\\nNUM_CHANNELS_LIGANDS = 16\\nNUM_CHANNELS_PROTEINS = 16\\n# Graph power to create bonds with after pooling.\\nGRAPH_POWER_AFTER_POOLING = 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gradient descent batch size.\n",
    "BATCH_SIZE = 32\n",
    "# Number of training epochs.\n",
    "NUM_EPOCHS = 10\n",
    "# Number of convolutional blocks.\n",
    "NUM_BLOCKS_LIGANDS = 1\n",
    "NUM_BLOCKS_PROTEINS = 1\n",
    "# Number of convolutional channels.\n",
    "NUM_CHANNELS_LIGANDS = 16\n",
    "NUM_CHANNELS_PROTEINS = 16\n",
    "# Graph power to create bonds with after pooling.\n",
    "GRAPH_POWER_AFTER_POOLING = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Load the list of targets, and generate the sublists of targets\\n# being trained and validated on. Since we set a seed, these lists\\n# will be the same as those which were used to generate the LigandDataset objects.\\nall_targets = pd.read_csv(\\\"../data/dud-e_targets.csv\\\").target_name.tolist()\\nall_targets = [target.lower() for target in all_targets]\\n\\nnp.random.seed(1)\\nnp.random.shuffle(all_targets)\\ntraining_targets = all_targets[:60]\\nvalidation_targets = all_targets[60:80]\\n\\n# Store these sublists in a dictionary.\\ntargets = {\\\"training\\\": training_targets, \\\"validation\\\": validation_targets}\";\n",
       "                var nbb_formatted_code = \"# Load the list of targets, and generate the sublists of targets\\n# being trained and validated on. Since we set a seed, these lists\\n# will be the same as those which were used to generate the LigandDataset objects.\\nall_targets = pd.read_csv(\\\"../data/dud-e_targets.csv\\\").target_name.tolist()\\nall_targets = [target.lower() for target in all_targets]\\n\\nnp.random.seed(1)\\nnp.random.shuffle(all_targets)\\ntraining_targets = all_targets[:60]\\nvalidation_targets = all_targets[60:80]\\n\\n# Store these sublists in a dictionary.\\ntargets = {\\\"training\\\": training_targets, \\\"validation\\\": validation_targets}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the list of targets, and generate the sublists of targets\n",
    "# being trained and validated on. Since we set a seed, these lists\n",
    "# will be the same as those which were used to generate the LigandDataset objects.\n",
    "all_targets = pd.read_csv(\"../data/dud-e_targets.csv\").target_name.tolist()\n",
    "all_targets = [target.lower() for target in all_targets]\n",
    "\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_targets)\n",
    "training_targets = all_targets[:60]\n",
    "validation_targets = all_targets[60:80]\n",
    "\n",
    "# Store these sublists in a dictionary.\n",
    "targets = {\"training\": training_targets, \"validation\": validation_targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# Load the target Data object for each target.\\ntarget_dict = dict(\\n    zip(\\n        all_targets,\\n        [\\n            pd.read_pickle(f\\\"../data/raw/{10}_bond_cutoff_{5}_{target}.pkl\\\")\\n            for target in all_targets\\n        ],\\n    )\\n)\";\n",
       "                var nbb_formatted_code = \"# Load the target Data object for each target.\\ntarget_dict = dict(\\n    zip(\\n        all_targets,\\n        [\\n            pd.read_pickle(f\\\"../data/raw/{10}_bond_cutoff_{5}_{target}.pkl\\\")\\n            for target in all_targets\\n        ],\\n    )\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the target Data object for each target.\n",
    "target_dict = dict(\n",
    "    zip(\n",
    "        all_targets,\n",
    "        [\n",
    "            pd.read_pickle(f\"../data/raw/{10}_bond_cutoff_{5}_{target}.pkl\")\n",
    "            for target in all_targets\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Load the ligand Dataset objects.\\ntraining_set = pd.read_pickle(\\\"../data/training_set.pkl\\\")\\nvalidation_set = pd.read_pickle(\\\"../data/validation_set.pkl\\\")\";\n",
       "                var nbb_formatted_code = \"# Load the ligand Dataset objects.\\ntraining_set = pd.read_pickle(\\\"../data/training_set.pkl\\\")\\nvalidation_set = pd.read_pickle(\\\"../data/validation_set.pkl\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the ligand Dataset objects.\n",
    "training_set = pd.read_pickle(\"../data/training_set.pkl\")\n",
    "validation_set = pd.read_pickle(\"../data/validation_set.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"def convert_edge_index(edge_index):\\n    \\\"\\\"\\\"Convert edge list to bonds list.\\\"\\\"\\\"\\n    edge_index_list = edge_index.numpy().tolist()\\n    return [\\n        [edge_index_list[0][i], edge_index_list[1][i]]\\n        for i in range(len(edge_index_list[0]))\\n    ]\\n\\n\\ndef convert_bonds(bonds):\\n    \\\"\\\"\\\"Convert bonds list to edge list.\\\"\\\"\\\"\\n    return torch.tensor(np.hsplit(np.array(bonds).transpose(), 1)[0])\";\n",
       "                var nbb_formatted_code = \"def convert_edge_index(edge_index):\\n    \\\"\\\"\\\"Convert edge list to bonds list.\\\"\\\"\\\"\\n    edge_index_list = edge_index.numpy().tolist()\\n    return [\\n        [edge_index_list[0][i], edge_index_list[1][i]]\\n        for i in range(len(edge_index_list[0]))\\n    ]\\n\\n\\ndef convert_bonds(bonds):\\n    \\\"\\\"\\\"Convert bonds list to edge list.\\\"\\\"\\\"\\n    return torch.tensor(np.hsplit(np.array(bonds).transpose(), 1)[0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_edge_index(edge_index):\n",
    "    \"\"\"Convert edge list to bonds list.\"\"\"\n",
    "    edge_index_list = edge_index.numpy().tolist()\n",
    "    return [\n",
    "        [edge_index_list[0][i], edge_index_list[1][i]]\n",
    "        for i in range(len(edge_index_list[0]))\n",
    "    ]\n",
    "\n",
    "\n",
    "def convert_bonds(bonds):\n",
    "    \"\"\"Convert bonds list to edge list.\"\"\"\n",
    "    return torch.tensor(np.hsplit(np.array(bonds).transpose(), 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"def add_bonds(num_atoms, edge_index, graph_power=GRAPH_POWER_AFTER_POOLING):\\n    \\\"\\\"\\\"Get bond list using the graph power method.\\\"\\\"\\\"\\n    original_bonds = convert_edge_index(edge_index)\\n    # We will append the bonds induced by graph power to\\n    # the processed_bonds list, instead of appending them to\\n    # original_bonds directly - that would result in mistakes.\\n    processed_bonds = original_bonds.copy()\\n    # Create a dictionary which maps each atom index to a list\\n    # with the indices of all of its neighbors.\\n    original_bonds_dict = dict()\\n    for i in range(len(original_bonds)):\\n        try:\\n            original_bonds_dict[original_bonds[i][0]].append(original_bonds[i][1])\\n        except:\\n            original_bonds_dict[original_bonds[i][0]] = [original_bonds[i][1]]\\n\\n    # For each atom, find its neighbors to the `graph_power`-th graph power.\\n    for atom_index in range(num_atoms):\\n        all_neighbors = []\\n        # First, we explore the neighbors of the atom's immediate\\n        # neighbors as the second graph-power neighbors. Next, we consider\\n        # their neighbors, and so on.\\n        try:\\n            neighbors_to_explore = original_bonds_dict[atom_index]\\n        except:\\n            continue\\n        for power in range(1, graph_power + 1):\\n            new_neighbors = set()\\n            for neighbor_atom_index in neighbors_to_explore:\\n                new_neighbors.update(original_bonds_dict[neighbor_atom_index])\\n            # Store the `power`-th graph power atom neighbors in the list.\\n            all_neighbors += neighbors_to_explore\\n            # Their neighbors are now the new neighbors to explore.\\n            neighbors_to_explore = list(new_neighbors)\\n            new_neighbors = set()\\n        all_neighbors = list(set(all_neighbors))\\n        try:\\n            all_neighbors.remove(atom_index)\\n        except:\\n            pass\\n        # Add the associated bonds to the bonds list.\\n        processed_bonds += [\\n            [atom_index, neighbor_index] for neighbor_index in all_neighbors\\n        ]\\n    return convert_bonds(processed_bonds)\";\n",
       "                var nbb_formatted_code = \"def add_bonds(num_atoms, edge_index, graph_power=GRAPH_POWER_AFTER_POOLING):\\n    \\\"\\\"\\\"Get bond list using the graph power method.\\\"\\\"\\\"\\n    original_bonds = convert_edge_index(edge_index)\\n    # We will append the bonds induced by graph power to\\n    # the processed_bonds list, instead of appending them to\\n    # original_bonds directly - that would result in mistakes.\\n    processed_bonds = original_bonds.copy()\\n    # Create a dictionary which maps each atom index to a list\\n    # with the indices of all of its neighbors.\\n    original_bonds_dict = dict()\\n    for i in range(len(original_bonds)):\\n        try:\\n            original_bonds_dict[original_bonds[i][0]].append(original_bonds[i][1])\\n        except:\\n            original_bonds_dict[original_bonds[i][0]] = [original_bonds[i][1]]\\n\\n    # For each atom, find its neighbors to the `graph_power`-th graph power.\\n    for atom_index in range(num_atoms):\\n        all_neighbors = []\\n        # First, we explore the neighbors of the atom's immediate\\n        # neighbors as the second graph-power neighbors. Next, we consider\\n        # their neighbors, and so on.\\n        try:\\n            neighbors_to_explore = original_bonds_dict[atom_index]\\n        except:\\n            continue\\n        for power in range(1, graph_power + 1):\\n            new_neighbors = set()\\n            for neighbor_atom_index in neighbors_to_explore:\\n                new_neighbors.update(original_bonds_dict[neighbor_atom_index])\\n            # Store the `power`-th graph power atom neighbors in the list.\\n            all_neighbors += neighbors_to_explore\\n            # Their neighbors are now the new neighbors to explore.\\n            neighbors_to_explore = list(new_neighbors)\\n            new_neighbors = set()\\n        all_neighbors = list(set(all_neighbors))\\n        try:\\n            all_neighbors.remove(atom_index)\\n        except:\\n            pass\\n        # Add the associated bonds to the bonds list.\\n        processed_bonds += [\\n            [atom_index, neighbor_index] for neighbor_index in all_neighbors\\n        ]\\n    return convert_bonds(processed_bonds)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_bonds(num_atoms, edge_index, graph_power=GRAPH_POWER_AFTER_POOLING):\n",
    "    \"\"\"Get bond list using the graph power method.\"\"\"\n",
    "    original_bonds = convert_edge_index(edge_index)\n",
    "    # We will append the bonds induced by graph power to\n",
    "    # the processed_bonds list, instead of appending them to\n",
    "    # original_bonds directly - that would result in mistakes.\n",
    "    processed_bonds = original_bonds.copy()\n",
    "    # Create a dictionary which maps each atom index to a list\n",
    "    # with the indices of all of its neighbors.\n",
    "    original_bonds_dict = dict()\n",
    "    for i in range(len(original_bonds)):\n",
    "        try:\n",
    "            original_bonds_dict[original_bonds[i][0]].append(original_bonds[i][1])\n",
    "        except:\n",
    "            original_bonds_dict[original_bonds[i][0]] = [original_bonds[i][1]]\n",
    "\n",
    "    # For each atom, find its neighbors to the `graph_power`-th graph power.\n",
    "    for atom_index in range(num_atoms):\n",
    "        all_neighbors = []\n",
    "        # First, we explore the neighbors of the atom's immediate\n",
    "        # neighbors as the second graph-power neighbors. Next, we consider\n",
    "        # their neighbors, and so on.\n",
    "        try:\n",
    "            neighbors_to_explore = original_bonds_dict[atom_index]\n",
    "        except:\n",
    "            continue\n",
    "        for power in range(1, graph_power + 1):\n",
    "            new_neighbors = set()\n",
    "            for neighbor_atom_index in neighbors_to_explore:\n",
    "                new_neighbors.update(original_bonds_dict[neighbor_atom_index])\n",
    "            # Store the `power`-th graph power atom neighbors in the list.\n",
    "            all_neighbors += neighbors_to_explore\n",
    "            # Their neighbors are now the new neighbors to explore.\n",
    "            neighbors_to_explore = list(new_neighbors)\n",
    "            new_neighbors = set()\n",
    "        all_neighbors = list(set(all_neighbors))\n",
    "        try:\n",
    "            all_neighbors.remove(atom_index)\n",
    "        except:\n",
    "            pass\n",
    "        # Add the associated bonds to the bonds list.\n",
    "        processed_bonds += [\n",
    "            [atom_index, neighbor_index] for neighbor_index in all_neighbors\n",
    "        ]\n",
    "    return convert_bonds(processed_bonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"class GCN(torch.nn.Module):\\n    def __init__(\\n        self,\\n        ligand_num_layers=NUM_BLOCKS_LIGANDS,\\n        ligand_hidden_channels=NUM_CHANNELS_LIGANDS,\\n        protein_num_layers=NUM_BLOCKS_PROTEINS,\\n        protein_hidden_channels=NUM_CHANNELS_PROTEINS,\\n    ):\\n        super(GCN, self).__init__()\\n        torch.manual_seed(1)\\n        # Create the convolutional layers.\\n        self.ligand_conv = [GCNConv(1, ligand_hidden_channels, improved=True)] + [\\n            GCNConv(ligand_hidden_channels, ligand_hidden_channels, improved=True)\\n            for _ in range(ligand_num_layers - 1)\\n        ]\\n        # Create the pooling layers.\\n        self.ligand_pool = [\\n            topk_pool.TopKPooling(ligand_hidden_channels)\\n            for _ in range(ligand_num_layers)\\n        ]\\n        # Create the convolutional layers.\\n        self.protein_conv = [GCNConv(3, protein_hidden_channels, improved=True)] + [\\n            GCNConv(protein_hidden_channels, protein_hidden_channels, improved=True)\\n            for _ in range(protein_num_layers - 1)\\n        ]\\n        # Create the pooling layers.\\n        self.protein_pool = [\\n            topk_pool.TopKPooling(protein_hidden_channels)\\n            for _ in range(protein_num_layers)\\n        ]\\n\\n        # self.dropout = Dropout(0.25)\\n        # Create the fully-connected layer.\\n        self.fc = Linear(ligand_hidden_channels + protein_hidden_channels, 2)\\n\\n        self.ligand_num_layers = ligand_num_layers\\n        self.protein_num_layers = protein_num_layers\\n\\n    def forward(self, ligand, protein):\\n        # Process ligand.\\n        x_l = ligand.x\\n        edge_index = ligand.edge_index\\n        edge_attr = torch.squeeze(ligand.edge_attr)\\n        # Pass the node and edge information through each convolutional block.\\n        for block in range(self.ligand_num_layers):\\n            # Update node representations through convolution.\\n            x_l = self.ligand_conv[block](x_l.float(), edge_index, edge_attr)\\n            # Apply non-linear activation.\\n            x_l = x_l.relu()\\n            # Pool the graph to 50% of its current size.\\n            x_l, edge_index, edge_attr, _, _, _ = self.ligand_pool[block](\\n                x_l, edge_index, edge_attr\\n            )\\n            # Add new bonds between the remaining nodes with graph power.\\n            edge_index = add_bonds(x_l.shape[0], edge_index)\\n        # Flatten the graph by taking the mean of each channel.\\n        x_l = global_mean_pool(x_l, batch=torch.LongTensor([0]))\\n\\n        # Process protein.\\n        x_p = protein.x\\n        edge_index = protein.edge_index\\n        edge_attr = torch.squeeze(protein.edge_attr)\\n        # Pass the node and edge information through each convolutional block.\\n        for block in range(self.protein_num_layers):\\n            # Update node representations through convolution.\\n            x_p = self.protein_conv[block](x_p.float(), edge_index, edge_attr)\\n            # Apply non-linear activation.\\n            x_p = x_p.relu()\\n            # Pool the graph to 50% of its current size.\\n            x_p, edge_index, edge_attr, _, _, _ = self.protein_pool[block](\\n                x_p, edge_index, edge_attr\\n            )\\n            # Add new bonds between the remaining nodes with graph power.\\n            edge_index = add_bonds(x_p.shape[0], edge_index)\\n        # Flatten the graph by taking the mean of each channel.\\n        x_p = global_mean_pool(x_p, batch=torch.LongTensor([0]))\\n\\n        # Concatenate ligand and protein representations.\\n        x = torch.cat([x_l[0], x_p[0]]).float()\\n        # x = self.dropout(x)\\n        # Pass the vector through a fully-connected layer before applying softmax activation.\\n        x = self.fc(x)\\n        return F.log_softmax(x, dim=0)\";\n",
       "                var nbb_formatted_code = \"class GCN(torch.nn.Module):\\n    def __init__(\\n        self,\\n        ligand_num_layers=NUM_BLOCKS_LIGANDS,\\n        ligand_hidden_channels=NUM_CHANNELS_LIGANDS,\\n        protein_num_layers=NUM_BLOCKS_PROTEINS,\\n        protein_hidden_channels=NUM_CHANNELS_PROTEINS,\\n    ):\\n        super(GCN, self).__init__()\\n        torch.manual_seed(1)\\n        # Create the convolutional layers.\\n        self.ligand_conv = [GCNConv(1, ligand_hidden_channels, improved=True)] + [\\n            GCNConv(ligand_hidden_channels, ligand_hidden_channels, improved=True)\\n            for _ in range(ligand_num_layers - 1)\\n        ]\\n        # Create the pooling layers.\\n        self.ligand_pool = [\\n            topk_pool.TopKPooling(ligand_hidden_channels)\\n            for _ in range(ligand_num_layers)\\n        ]\\n        # Create the convolutional layers.\\n        self.protein_conv = [GCNConv(3, protein_hidden_channels, improved=True)] + [\\n            GCNConv(protein_hidden_channels, protein_hidden_channels, improved=True)\\n            for _ in range(protein_num_layers - 1)\\n        ]\\n        # Create the pooling layers.\\n        self.protein_pool = [\\n            topk_pool.TopKPooling(protein_hidden_channels)\\n            for _ in range(protein_num_layers)\\n        ]\\n\\n        # self.dropout = Dropout(0.25)\\n        # Create the fully-connected layer.\\n        self.fc = Linear(ligand_hidden_channels + protein_hidden_channels, 2)\\n\\n        self.ligand_num_layers = ligand_num_layers\\n        self.protein_num_layers = protein_num_layers\\n\\n    def forward(self, ligand, protein):\\n        # Process ligand.\\n        x_l = ligand.x\\n        edge_index = ligand.edge_index\\n        edge_attr = torch.squeeze(ligand.edge_attr)\\n        # Pass the node and edge information through each convolutional block.\\n        for block in range(self.ligand_num_layers):\\n            # Update node representations through convolution.\\n            x_l = self.ligand_conv[block](x_l.float(), edge_index, edge_attr)\\n            # Apply non-linear activation.\\n            x_l = x_l.relu()\\n            # Pool the graph to 50% of its current size.\\n            x_l, edge_index, edge_attr, _, _, _ = self.ligand_pool[block](\\n                x_l, edge_index, edge_attr\\n            )\\n            # Add new bonds between the remaining nodes with graph power.\\n            edge_index = add_bonds(x_l.shape[0], edge_index)\\n        # Flatten the graph by taking the mean of each channel.\\n        x_l = global_mean_pool(x_l, batch=torch.LongTensor([0]))\\n\\n        # Process protein.\\n        x_p = protein.x\\n        edge_index = protein.edge_index\\n        edge_attr = torch.squeeze(protein.edge_attr)\\n        # Pass the node and edge information through each convolutional block.\\n        for block in range(self.protein_num_layers):\\n            # Update node representations through convolution.\\n            x_p = self.protein_conv[block](x_p.float(), edge_index, edge_attr)\\n            # Apply non-linear activation.\\n            x_p = x_p.relu()\\n            # Pool the graph to 50% of its current size.\\n            x_p, edge_index, edge_attr, _, _, _ = self.protein_pool[block](\\n                x_p, edge_index, edge_attr\\n            )\\n            # Add new bonds between the remaining nodes with graph power.\\n            edge_index = add_bonds(x_p.shape[0], edge_index)\\n        # Flatten the graph by taking the mean of each channel.\\n        x_p = global_mean_pool(x_p, batch=torch.LongTensor([0]))\\n\\n        # Concatenate ligand and protein representations.\\n        x = torch.cat([x_l[0], x_p[0]]).float()\\n        # x = self.dropout(x)\\n        # Pass the vector through a fully-connected layer before applying softmax activation.\\n        x = self.fc(x)\\n        return F.log_softmax(x, dim=0)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ligand_num_layers=NUM_BLOCKS_LIGANDS,\n",
    "        ligand_hidden_channels=NUM_CHANNELS_LIGANDS,\n",
    "        protein_num_layers=NUM_BLOCKS_PROTEINS,\n",
    "        protein_hidden_channels=NUM_CHANNELS_PROTEINS,\n",
    "    ):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(1)\n",
    "        # Create the convolutional layers.\n",
    "        self.ligand_conv = [GCNConv(1, ligand_hidden_channels, improved=True)] + [\n",
    "            GCNConv(ligand_hidden_channels, ligand_hidden_channels, improved=True)\n",
    "            for _ in range(ligand_num_layers - 1)\n",
    "        ]\n",
    "        # Create the pooling layers.\n",
    "        self.ligand_pool = [\n",
    "            topk_pool.TopKPooling(ligand_hidden_channels)\n",
    "            for _ in range(ligand_num_layers)\n",
    "        ]\n",
    "        # Create the convolutional layers.\n",
    "        self.protein_conv = [GCNConv(3, protein_hidden_channels, improved=True)] + [\n",
    "            GCNConv(protein_hidden_channels, protein_hidden_channels, improved=True)\n",
    "            for _ in range(protein_num_layers - 1)\n",
    "        ]\n",
    "        # Create the pooling layers.\n",
    "        self.protein_pool = [\n",
    "            topk_pool.TopKPooling(protein_hidden_channels)\n",
    "            for _ in range(protein_num_layers)\n",
    "        ]\n",
    "\n",
    "        # self.dropout = Dropout(0.25)\n",
    "        # Create the fully-connected layer.\n",
    "        self.fc = Linear(ligand_hidden_channels + protein_hidden_channels, 2)\n",
    "\n",
    "        self.ligand_num_layers = ligand_num_layers\n",
    "        self.protein_num_layers = protein_num_layers\n",
    "\n",
    "    def forward(self, ligand, protein):\n",
    "        # Process ligand.\n",
    "        x_l = ligand.x\n",
    "        edge_index = ligand.edge_index\n",
    "        edge_attr = torch.squeeze(ligand.edge_attr)\n",
    "        # Pass the node and edge information through each convolutional block.\n",
    "        for block in range(self.ligand_num_layers):\n",
    "            # Update node representations through convolution.\n",
    "            x_l = self.ligand_conv[block](x_l.float(), edge_index, edge_attr)\n",
    "            # Apply non-linear activation.\n",
    "            x_l = x_l.relu()\n",
    "            # Pool the graph to 50% of its current size.\n",
    "            x_l, edge_index, edge_attr, _, _, _ = self.ligand_pool[block](\n",
    "                x_l, edge_index, edge_attr\n",
    "            )\n",
    "            # Add new bonds between the remaining nodes with graph power.\n",
    "            edge_index = add_bonds(x_l.shape[0], edge_index)\n",
    "        # Flatten the graph by taking the mean of each channel.\n",
    "        x_l = global_mean_pool(x_l, batch=torch.LongTensor([0]))\n",
    "\n",
    "        # Process protein.\n",
    "        x_p = protein.x\n",
    "        edge_index = protein.edge_index\n",
    "        edge_attr = torch.squeeze(protein.edge_attr)\n",
    "        # Pass the node and edge information through each convolutional block.\n",
    "        for block in range(self.protein_num_layers):\n",
    "            # Update node representations through convolution.\n",
    "            x_p = self.protein_conv[block](x_p.float(), edge_index, edge_attr)\n",
    "            # Apply non-linear activation.\n",
    "            x_p = x_p.relu()\n",
    "            # Pool the graph to 50% of its current size.\n",
    "            x_p, edge_index, edge_attr, _, _, _ = self.protein_pool[block](\n",
    "                x_p, edge_index, edge_attr\n",
    "            )\n",
    "            # Add new bonds between the remaining nodes with graph power.\n",
    "            edge_index = add_bonds(x_p.shape[0], edge_index)\n",
    "        # Flatten the graph by taking the mean of each channel.\n",
    "        x_p = global_mean_pool(x_p, batch=torch.LongTensor([0]))\n",
    "\n",
    "        # Concatenate ligand and protein representations.\n",
    "        x = torch.cat([x_l[0], x_p[0]]).float()\n",
    "        # x = self.dropout(x)\n",
    "        # Pass the vector through a fully-connected layer before applying softmax activation.\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"def get_pred(out):\\n    \\\"\\\"\\\"Return the class predicted most likely.\\\"\\\"\\\"\\n    return int((out[0] < out[1]).item())\\n\\n\\ndef get_accuracy(all_truths, all_preds):\\n    \\\"\\\"\\\"Return accuracy.\\\"\\\"\\\"\\n    return np.mean(np.array(all_truths) == np.array(all_preds))\\n\\n\\ndef get_prob(out):\\n    \\\"\\\"\\\"Return the predicted probability of the positive class.\\\"\\\"\\\"\\n    return np.exp(out[1].item())\\n\\n\\ndef train():\\n    \\\"\\\"\\\"Train the model and print epoch performance.\\\"\\\"\\\"\\n    model.train()\\n\\n    all_preds = []\\n    all_preds_ = []\\n    all_truths = []\\n    # Pass through every example in the training set.\\n    for i in range(training_set.len()):\\n        curr_ligand = training_set.get(i)\\n        curr_response = curr_ligand.y\\n        curr_target = target_dict[curr_ligand.target]\\n\\n        # Pass the current ligand and target data through the model.\\n        out = model(curr_ligand, curr_target)\\n        # Compute the loss.\\n        loss = criterion(out.unsqueeze(0), curr_ligand.y[0])\\n        all_preds.append(get_pred(out))\\n        all_preds_.append(get_prob(out))\\n        all_truths.append(curr_ligand.y[0].item())\\n        # Derive gradients.\\n        loss.backward()\\n        # If the sufficient number of steps before updating have been\\n        # reached, update the weights.\\n        if i > 0 and not i % BATCH_SIZE:\\n            # Update parameters based on gradients.\\n            optimizer.step()\\n            # Clear gradients.\\n            optimizer.zero_grad()\\n    print(\\n        f\\\"Training AUC: {roc_auc_score(all_truths, all_preds_)},\\\"\\n        f\\\" precision: {precision_score(all_truths, all_preds)},\\\"\\n        f\\\" recall: {recall_score(all_truths, all_preds)},\\\"\\n        f\\\" accuracy: {get_accuracy(all_truths, all_preds)}\\\"\\n    )\\n\\n\\ndef test():\\n    \\\"\\\"\\\"Evaluate the model on the validation set.\\\"\\\"\\\"\\n    model.eval()\\n\\n    all_preds = []\\n    all_preds_ = []\\n    all_truths = []\\n    # Pass through every example in the training set.\\n    for i in range(validation_set.len()):\\n        curr_ligand = validation_set.get(i)\\n        curr_response = curr_ligand.y\\n        curr_target = target_dict[curr_ligand.target]\\n\\n        # Perform a single forward pass.\\n        out = model(curr_ligand, curr_target)\\n        all_preds.append(get_pred(out))\\n        all_preds_.append(get_prob(out))\\n        all_truths.append(curr_ligand.y[0].item())\\n    print(\\n        f\\\"Validation AUC: {roc_auc_score(all_truths, all_preds_)},\\\"\\n        f\\\" precision: {precision_score(all_truths, all_preds)},\\\"\\n        f\\\" recall: {recall_score(all_truths, all_preds)},\\\"\\n        f\\\" accuracy: {get_accuracy(all_truths, all_preds)}\\\"\\n    )\";\n",
       "                var nbb_formatted_code = \"def get_pred(out):\\n    \\\"\\\"\\\"Return the class predicted most likely.\\\"\\\"\\\"\\n    return int((out[0] < out[1]).item())\\n\\n\\ndef get_accuracy(all_truths, all_preds):\\n    \\\"\\\"\\\"Return accuracy.\\\"\\\"\\\"\\n    return np.mean(np.array(all_truths) == np.array(all_preds))\\n\\n\\ndef get_prob(out):\\n    \\\"\\\"\\\"Return the predicted probability of the positive class.\\\"\\\"\\\"\\n    return np.exp(out[1].item())\\n\\n\\ndef train():\\n    \\\"\\\"\\\"Train the model and print epoch performance.\\\"\\\"\\\"\\n    model.train()\\n\\n    all_preds = []\\n    all_preds_ = []\\n    all_truths = []\\n    # Pass through every example in the training set.\\n    for i in range(training_set.len()):\\n        curr_ligand = training_set.get(i)\\n        curr_response = curr_ligand.y\\n        curr_target = target_dict[curr_ligand.target]\\n\\n        # Pass the current ligand and target data through the model.\\n        out = model(curr_ligand, curr_target)\\n        # Compute the loss.\\n        loss = criterion(out.unsqueeze(0), curr_ligand.y[0])\\n        all_preds.append(get_pred(out))\\n        all_preds_.append(get_prob(out))\\n        all_truths.append(curr_ligand.y[0].item())\\n        # Derive gradients.\\n        loss.backward()\\n        # If the sufficient number of steps before updating have been\\n        # reached, update the weights.\\n        if i > 0 and not i % BATCH_SIZE:\\n            # Update parameters based on gradients.\\n            optimizer.step()\\n            # Clear gradients.\\n            optimizer.zero_grad()\\n    print(\\n        f\\\"Training AUC: {roc_auc_score(all_truths, all_preds_)},\\\"\\n        f\\\" precision: {precision_score(all_truths, all_preds)},\\\"\\n        f\\\" recall: {recall_score(all_truths, all_preds)},\\\"\\n        f\\\" accuracy: {get_accuracy(all_truths, all_preds)}\\\"\\n    )\\n\\n\\ndef test():\\n    \\\"\\\"\\\"Evaluate the model on the validation set.\\\"\\\"\\\"\\n    model.eval()\\n\\n    all_preds = []\\n    all_preds_ = []\\n    all_truths = []\\n    # Pass through every example in the training set.\\n    for i in range(validation_set.len()):\\n        curr_ligand = validation_set.get(i)\\n        curr_response = curr_ligand.y\\n        curr_target = target_dict[curr_ligand.target]\\n\\n        # Perform a single forward pass.\\n        out = model(curr_ligand, curr_target)\\n        all_preds.append(get_pred(out))\\n        all_preds_.append(get_prob(out))\\n        all_truths.append(curr_ligand.y[0].item())\\n    print(\\n        f\\\"Validation AUC: {roc_auc_score(all_truths, all_preds_)},\\\"\\n        f\\\" precision: {precision_score(all_truths, all_preds)},\\\"\\n        f\\\" recall: {recall_score(all_truths, all_preds)},\\\"\\n        f\\\" accuracy: {get_accuracy(all_truths, all_preds)}\\\"\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_pred(out):\n",
    "    \"\"\"Return the class predicted most likely.\"\"\"\n",
    "    return int((out[0] < out[1]).item())\n",
    "\n",
    "\n",
    "def get_accuracy(all_truths, all_preds):\n",
    "    \"\"\"Return accuracy.\"\"\"\n",
    "    return np.mean(np.array(all_truths) == np.array(all_preds))\n",
    "\n",
    "\n",
    "def get_prob(out):\n",
    "    \"\"\"Return the predicted probability of the positive class.\"\"\"\n",
    "    return np.exp(out[1].item())\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"Train the model and print epoch performance.\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    all_preds = []\n",
    "    all_preds_ = []\n",
    "    all_truths = []\n",
    "    # Pass through every example in the training set.\n",
    "    for i in range(training_set.len()):\n",
    "        curr_ligand = training_set.get(i)\n",
    "        curr_response = curr_ligand.y\n",
    "        curr_target = target_dict[curr_ligand.target]\n",
    "\n",
    "        # Pass the current ligand and target data through the model.\n",
    "        out = model(curr_ligand, curr_target)\n",
    "        # Compute the loss.\n",
    "        loss = criterion(out.unsqueeze(0), curr_ligand.y[0])\n",
    "        all_preds.append(get_pred(out))\n",
    "        all_preds_.append(get_prob(out))\n",
    "        all_truths.append(curr_ligand.y[0].item())\n",
    "        # Derive gradients.\n",
    "        loss.backward()\n",
    "        # If the sufficient number of steps before updating have been\n",
    "        # reached, update the weights.\n",
    "        if i > 0 and not i % BATCH_SIZE:\n",
    "            # Update parameters based on gradients.\n",
    "            optimizer.step()\n",
    "            # Clear gradients.\n",
    "            optimizer.zero_grad()\n",
    "    print(\n",
    "        f\"Training AUC: {roc_auc_score(all_truths, all_preds_)},\"\n",
    "        f\" precision: {precision_score(all_truths, all_preds)},\"\n",
    "        f\" recall: {recall_score(all_truths, all_preds)},\"\n",
    "        f\" accuracy: {get_accuracy(all_truths, all_preds)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"Evaluate the model on the validation set.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_preds_ = []\n",
    "    all_truths = []\n",
    "    # Pass through every example in the training set.\n",
    "    for i in range(validation_set.len()):\n",
    "        curr_ligand = validation_set.get(i)\n",
    "        curr_response = curr_ligand.y\n",
    "        curr_target = target_dict[curr_ligand.target]\n",
    "\n",
    "        # Perform a single forward pass.\n",
    "        out = model(curr_ligand, curr_target)\n",
    "        all_preds.append(get_pred(out))\n",
    "        all_preds_.append(get_prob(out))\n",
    "        all_truths.append(curr_ligand.y[0].item())\n",
    "    print(\n",
    "        f\"Validation AUC: {roc_auc_score(all_truths, all_preds_)},\"\n",
    "        f\" precision: {precision_score(all_truths, all_preds)},\"\n",
    "        f\" recall: {recall_score(all_truths, all_preds)},\"\n",
    "        f\" accuracy: {get_accuracy(all_truths, all_preds)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"model = GCN()\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\\ncriterion = torch.nn.NLLLoss()\";\n",
       "                var nbb_formatted_code = \"model = GCN()\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\\ncriterion = torch.nn.NLLLoss()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GCN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "def get_time_left(time_taken, epoch):\n",
    "    \"\"\"Get estimate of training and evaluation time left.\"\"\"\n",
    "    return round(((NUM_EPOCHS - epoch) * time_taken) / 60)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    start = time.time()\n",
    "    train()\n",
    "    test()\n",
    "    end = time.time()\n",
    "    print(get_time_left(end - start, epoch))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
